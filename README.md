âš¡ Event-Driven Architecture on AWS â€” S3 â†’ SNS â†’ Lambda â†’ DynamoDB

This project demonstrates a fully automated, serverless pipeline using AWS services to handle file uploads and store metadata in real time. Itâ€™s part of my cloud engineering portfolio, focused on building scalable, decoupled systems using event-driven design patterns.

ðŸŽ¯ Project Goal
Automatically capture and store metadata (filename, size, timestamp) of any file uploaded to an S3 bucket â€” without manual intervention or servers.
ðŸ”§ AWS Services Used
- Amazon S3: Triggers events on file uploads
- Amazon SNS: Publishes notifications to subscribed Lambda
- AWS Lambda: Extracts file metadata and formats it
- Amazon DynamoDB: Stores metadata in a scalable NoSQL table
- Amazon CloudWatch: Monitors logs and system health
- 
âœ… Key Learnings
- Designing event-driven, decoupled architectures
- Configuring IAM roles for secure service communication
- Debugging with CloudWatch Logs
- Building serverless automation pipelines with zero infrastructure management
- 
ðŸ“Ž Project Assets
- Architecture diagram
- Lambda function code
- IAM policy snippets
- Sample metadata entries in DynamoDB
- 
ðŸš€ Whatâ€™s Next
Iâ€™ll be integrating CloudWatch alarms and SNS email alerts to notify me of any failures in the pipeline â€” adding observability for production readiness.
